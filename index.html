<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Xiaoxi Yang</title>
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, sans-serif;
            color: #5a5a5a;
            background: #f7f5f2;
            line-height: 1.6;
        }

        /* Tab Navigation */
        .tab-bar {
            position: fixed;
            top: 0;
            width: 100%;
            background: rgba(208, 220, 222, 0.95);
            backdrop-filter: blur(12px);
            z-index: 100;
            display: flex;
            justify-content: center;
            gap: 0;
            border-bottom: 1px solid #d8d4cf;
        }
        .tab-btn {
            padding: 16px 28px;
            background: none;
            border: none;
            color: #6b7578;
            font-size: 15px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            border-bottom: 2px solid transparent;
            font-family: inherit;
        }
        .tab-btn:hover {
            color: #6b8f94;
        }
        .tab-btn.active {
            color: #5a7f84;
            border-bottom-color: #8aacb0;
            font-weight: 600;
        }

        /* Tab Content */
        .tab-content {
            display: none;
            padding-top: 56px;
            min-height: 100vh;
        }
        .tab-content.active { display: block; }

        /* Common section styling */
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 48px 32px 80px;
        }

        h2 {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 12px;
            color: #4a4545;
        }
        h3 {
            font-size: 20px;
            font-weight: 600;
            margin-bottom: 10px;
            color: #5a5252;
        }
        h4 {
            font-size: 15px;
            font-weight: 600;
            margin-bottom: 6px;
            color: #6b6363;
            text-align: center;
        }
        .section-subtitle {
            font-size: 17px;
            color: #8a8280;
            margin-bottom: 28px;
        }
        p { margin-bottom: 14px; font-size: 16px; }

        /* ==================== ABOUT ME ==================== */
        .about-hero {
            display: flex;
            gap: 40px;
            align-items: center;
            padding: 40px 0 36px;
        }
        .about-photo {
            width: 180px;
            height: 180px;
            border-radius: 50%;
            object-fit: cover;
            flex-shrink: 0;
            border: 4px solid #c5d5d8;
        }
        .about-info h1 {
            font-size: 36px;
            font-weight: 700;
            margin-bottom: 6px;
            color: #4a4545;
        }
        .about-info .contact-links {
            display: flex;
            gap: 12px;
            margin-top: 8px;
        }
        .icon-btn {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background: #e0eaec;
            color: #6b7f8a;
            text-decoration: none;
            transition: all 0.2s;
        }
        .icon-btn:hover {
            background: #8aacb0;
            color: #fff;
        }

        .edu-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 24px 0;
        }
        .edu-card {
            background: #e8eff0;
            border-radius: 10px;
            padding: 22px;
            border-left: 4px solid #8aacb0;
        }
        .edu-card .school {
            font-weight: 700;
            font-size: 17px;
            color: #4a5558;
        }
        .edu-card .degree {
            font-size: 14px;
            color: #6b6363;
            margin: 4px 0;
        }
        .edu-card .detail {
            font-size: 13px;
            color: #9a9490;
        }

        .interest-text {
            font-size: 17px;
            color: #5a5a5a;
            line-height: 1.85;
            margin: 20px 0;
            padding: 28px 32px;
            background: #e8eff0;
            border-radius: 12px;
        }
        .interest-text a {
            color: #2a6269;
            text-decoration: none;
            font-weight: 600;
        }
        .interest-text a:hover {
            text-decoration: underline;
        }

        .skills-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 16px;
            margin-top: 20px;
        }
        .skill-tag {
            background: #f8f9fa;
            padding: 12px 16px;
            border-radius: 8px;
            text-align: center;
        }
        .skill-tag .label {
            font-size: 12px;
            color: #888;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .skill-tag .value {
            font-size: 14px;
            font-weight: 600;
            color: #2c3e50;
            margin-top: 4px;
        }

        /* ==================== CARDS ==================== */
        .card {
            background: #e8eff0;
            border-radius: 10px;
            padding: 24px;
            margin-bottom: 20px;
        }
        .card-blue { background: #e4ecee; border-left: 4px solid #8aacb0; }
        .card-green { background: #e4ede8; border-left: 4px solid #88ada0; }
        .card-orange { background: #eaece4; border-left: 4px solid #a8b88a; }
        .card-red { background: #eae6e8; border-left: 4px solid #b09aaa; }

        /* Pipeline */
        .pipeline {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 8px;
            flex-wrap: wrap;
            margin: 28px 0;
        }
        .pipeline-step {
            background: #8aacb0;
            color: white;
            padding: 12px 18px;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 500;
            text-align: center;
        }
        .pipeline-step.highlight { background: #88ada0; }
        .pipeline-arrow { font-size: 22px; color: #b0c4c6; }

        /* Viz grid */
        .viz-grid {
            display: grid;
            gap: 16px;
            margin: 20px 0;
        }
        .viz-grid-2 { grid-template-columns: 1fr 1fr; }
        .viz-grid-3 { grid-template-columns: 1fr 1fr 1fr; }
        .viz-grid-4 { grid-template-columns: 1fr 1fr 1fr 1fr; }
        .viz-item {
            background: #f5f8f8;
            border: 1px solid #d0dcde;
            border-radius: 8px;
            padding: 10px;
            text-align: center;
        }
        .viz-item img { width: 100%; border-radius: 4px; }
        model-viewer {
            width: 100%;
            height: 280px;
            border-radius: 4px;
            background: #f0f4f4;
        }

        /* Stats */
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 16px;
            margin: 20px 0;
        }
        .stat-card {
            text-align: center;
            padding: 20px;
            background: #e8eff0;
            border-radius: 10px;
        }
        .stat-value { font-size: 32px; font-weight: 700; color: #6b8f94; }
        .stat-value.green { color: #6a9a82; }
        .stat-value.red { color: #a08a9a; }
        .stat-label { font-size: 13px; color: #8a9598; margin-top: 4px; }

        .badge {
            display: inline-block;
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 13px;
            font-weight: 600;
        }
        .badge-green { background: #dceae2; color: #5a8a6e; }
        .badge-red { background: #e6dee4; color: #8a7088; }

        /* Insight callout */
        .insight {
            background: linear-gradient(135deg, #7a9ea8 0%, #88ada0 100%);
            color: white;
            padding: 22px 26px;
            border-radius: 10px;
            margin: 20px 0;
            font-size: 16px;
        }
        .insight strong { color: #fff; }

        /* Constraint stages */
        .stage-row {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 14px;
            margin: 14px 0;
        }
        .stage-item { text-align: center; }
        .stage-item img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #d0dcde;
        }
        .stage-label { font-size: 12px; color: #8a9598; margin-top: 4px; }

        /* ==================== PROJECTS ==================== */
        .project-card {
            background: #f5f8f8;
            border: 1px solid #d0dcde;
            border-radius: 12px;
            padding: 28px;
            margin-bottom: 24px;
            transition: box-shadow 0.2s;
        }
        .project-card:hover { box-shadow: 0 4px 16px rgba(0,0,0,0.05); }
        .project-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 12px;
        }
        .project-header h3 { margin-bottom: 0; }
        .project-date {
            font-size: 13px;
            color: #9a9490;
            white-space: nowrap;
        }
        .project-tech {
            display: flex;
            gap: 6px;
            flex-wrap: wrap;
            margin-bottom: 12px;
        }
        .tech-tag {
            background: #dde8ea;
            color: #5e7a7e;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 500;
        }
        .project-card ul {
            margin-left: 18px;
            font-size: 15px;
            color: #5a5a5a;
        }
        .project-card li { margin-bottom: 6px; }

        /* ==================== PUBLICATIONS ==================== */
        .pub-card {
            background: #e8eff0;
            border-radius: 10px;
            padding: 22px;
            margin-bottom: 16px;
            border-left: 4px solid #8aacb0;
        }
        .pub-title {
            font-weight: 600;
            font-size: 15px;
            color: #4a4545;
            margin-bottom: 4px;
        }
        .pub-authors {
            font-size: 14px;
            color: #7a7270;
            margin-bottom: 4px;
        }
        .pub-venue {
            font-size: 13px;
            color: #9a9490;
            font-style: italic;
        }

        /* ==================== MISC ==================== */
        .direction-card {
            display: flex;
            gap: 14px;
            align-items: flex-start;
            padding: 18px;
            background: #e8eff0;
            border-radius: 10px;
            margin-bottom: 14px;
        }
        .direction-icon {
            font-size: 24px;
            flex-shrink: 0;
            width: 36px;
            height: 36px;
            background: #8aacb0;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 16px;
            font-weight: 700;
        }

        /* Assignment cards with images */
        .assignment-card {
            display: flex;
            gap: 20px;
            align-items: center;
            background: #e8eff0;
            border-radius: 10px;
            padding: 16px;
            margin-bottom: 16px;
            border-left: 4px solid #88ada0;
        }
        .assignment-img {
            width: 120px;
            height: 120px;
            object-fit: cover;
            border-radius: 8px;
            flex-shrink: 0;
        }

        /* Lightbox overlay */
        .lightbox-overlay {
            display: none;
            position: fixed;
            top: 0; left: 0;
            width: 100%; height: 100%;
            background: rgba(0,0,0,0.85);
            z-index: 1000;
            cursor: zoom-out;
            justify-content: center;
            align-items: center;
        }
        .lightbox-overlay.active { display: flex; }
        .lightbox-overlay img,
        .lightbox-overlay video {
            max-width: 90vw;
            max-height: 90vh;
            border-radius: 8px;
            object-fit: contain;
        }
        .assignment-img, .project-card img {
            cursor: zoom-in;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .assignment-img:hover, .project-card img:hover {
            transform: scale(1.03);
            box-shadow: 0 4px 16px rgba(0,0,0,0.15);
        }
        .img-wrapper {
            position: relative;
            display: inline-block;
            flex-shrink: 0;
        }
        .img-wrapper::after {
            content: "Click to enlarge";
            position: absolute;
            bottom: 6px;
            right: 6px;
            background: rgba(0,0,0,0.6);
            color: #fff;
            font-size: 11px;
            padding: 3px 8px;
            border-radius: 4px;
            pointer-events: none;
        }

        /* Section divider within tab */
        .section-gap {
            height: 1px;
            background: #d0dcde;
            margin: 36px 0;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .about-hero { flex-direction: column; text-align: center; }
            .edu-grid { grid-template-columns: 1fr; }
            .viz-grid-3, .viz-grid-4 { grid-template-columns: 1fr 1fr; }
            .stats-grid { grid-template-columns: 1fr; }
            .pipeline { flex-direction: column; }
            .skills-grid { grid-template-columns: 1fr 1fr; }
            .tab-btn { padding: 14px 16px; font-size: 13px; }
        }
    </style>
</head>
<body>

<!-- Tab Bar -->
<div class="tab-bar">
    <button class="tab-btn active" onclick="switchTab('about')">About Me</button>
    <button class="tab-btn" onclick="switchTab('research')">Research</button>
    <button class="tab-btn" onclick="switchTab('projects')">Projects</button>
    <button class="tab-btn" onclick="switchTab('publications')">Publications</button>
</div>

<!-- ==================== TAB 1: ABOUT ME ==================== -->
<div class="tab-content active" id="tab-about">
<div class="container">

    <div class="about-hero">
        <img class="about-photo" src="photo.jpg" alt="Xiaoxi Yang">
        <div class="about-info">
            <h1>Xiaoxi Yang</h1>
            <div class="contact-links">
                <div style="display:flex; flex-direction:column; align-items:center; gap:4px;">
                    <a href="mailto:xiaoxi_yang@brown.edu" class="icon-btn" title="Email">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="4" width="20" height="16" rx="2"/><path d="M22 4L12 13 2 4"/></svg>
                    </a>
                    <span style="font-size:11px; color:#999;">Email</span>
                </div>
                <div style="display:flex; flex-direction:column; align-items:center; gap:4px;">
                    <a href="https://github.com/yangxiaoxi65" class="icon-btn" title="GitHub">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/></svg>
                    </a>
                    <span style="font-size:11px; color:#999;">GitHub</span>
                </div>
                <div style="display:flex; flex-direction:column; align-items:center; gap:4px;">
                    <a href="CV.pdf" class="icon-btn" target="_blank" rel="noopener" title="CV (last updated Feb 2026)">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6a2 2 0 00-2 2v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
                    </a>
                    <span style="font-size:11px; color:#999;">CV (Feb 2026)</span>
                </div>
            </div>
        </div>
    </div>

    <div class="interest-text">
        I am a research assistant at Brown University, working with
        <a href="https://dritchie.github.io/" target="_blank">Prof. Daniel Ritchie</a> and collaborating with
        <a href="https://www-sop.inria.fr/members/Adrien.Bousseau/" target="_blank">Dr. Adrien Bousseau</a> at Inria.
        I received my M.S. in Computer Science from Brown University and my B.S. in Computer Science with a minor in Mathematics from Wenzhou-Kean University.
        My research interest lies in computer graphics, with a focus on applying AI and machine learning to 3D content creation.
        I am particularly interested in building tools that make it easier for people to create and work with 3D content.
        Outside of research, I enjoy traveling and baking.
    </div>


</div>
</div>

<!-- ==================== TAB 2: RESEARCH ==================== -->
<div class="tab-content" id="tab-research">
<div class="container">

    <h2>2D-to-3D Sketch Lifting</h2>
    <p class="section-subtitle">Combining neural depth estimation with geometric reasoning for 3D stroke reconstruction</p>
    <p style="font-size:16px; color:#5a5a5a; margin-bottom:24px;">Advisors: Prof. Daniel Ritchie (Brown) &amp; Dr. Adrien Bousseau (Inria) &middot; Jun 2025 - Present</p>

    <!-- Background -->
    <div class="card card-blue">
        <h3>Motivation</h3>
        <p>Designers communicate 3D ideas through 2D sketches. <strong>True2Form</strong> (Bousseau et al.) lifts 2D design sketches to 3D using geometric heuristics (symmetry, parallelism), but struggles with complex inputs. Meanwhile, <strong>large reconstruction models</strong> like VGGT predict 3D from images but were trained on photographs, not line drawings.</p>
        <p style="margin-bottom:0;"><strong>Research question:</strong> Can we combine neural depth estimation with classical geometric constraints to get the best of both worlds?</p>
    </div>

    <!-- Pipeline -->
    <h3>Pipeline Overview</h3>
    <div class="pipeline">
        <div class="pipeline-step">2D Sketch</div>
        <div class="pipeline-arrow">&rarr;</div>
        <div class="pipeline-step">VGGT Depth</div>
        <div class="pipeline-arrow">&rarr;</div>
        <div class="pipeline-step">Back-projection</div>
        <div class="pipeline-arrow">&rarr;</div>
        <div class="pipeline-step">3D Strokes</div>
        <div class="pipeline-arrow">&rarr;</div>
        <div class="pipeline-step highlight">Geometric Constraints</div>
    </div>

    <div class="section-gap"></div>

    <!-- Stage 1: Baseline -->
    <h2>Stage 1: VGGT Baseline</h2>
    <p class="section-subtitle">Validating neural depth prediction on line drawings</p>

    <div class="card card-green">
        <p><strong>Result:</strong> VGGT successfully predicts depth from line drawings, even though it was trained on photographs. We validated on 100 synthetic CAD models.</p>
    </div>

    <h3>Depth Prediction &amp; 3D Stroke Fitting</h3>
    <div class="viz-grid viz-grid-3">
        <div class="viz-item">
            <h4>2D Input</h4>
            <img src="https://yangxiaoxi65.github.io/VGGT-3D-Stroke-Fitting-Results/item_0/projection.png" alt="Input sketch">
        </div>
        <div class="viz-item">
            <h4>Ground Truth 3D</h4>
            <model-viewer
                src="https://yangxiaoxi65.github.io/VGGT-3D-Stroke-Fitting-Results/item_0/ground_truth.glb"
                camera-controls camera-orbit="45deg 55deg auto" bounds="tight"
            ></model-viewer>
        </div>
        <div class="viz-item">
            <h4>VGGT 3D Strokes</h4>
            <model-viewer
                src="https://yangxiaoxi65.github.io/VGGT-3D-Stroke-Fitting-Results/item_0/vggt_reconstruction.glb"
                camera-controls camera-orbit="45deg 55deg auto" bounds="tight"
            ></model-viewer>
        </div>
    </div>

    <div class="card card-orange" style="margin-top: 20px;">
        <p style="margin-bottom:0;"><strong>Challenge:</strong> Reconstructed strokes show geometric imperfections: strokes that should be parallel aren't quite parallel, and endpoints that should connect have small gaps. This motivates the next stage: can we fix these imperfections using geometric reasoning?</p>
    </div>

    <div class="section-gap"></div>

    <!-- Stage 2: Constraints -->
    <h2>Stage 2: Geometric Constraint Optimization</h2>
    <p class="section-subtitle">Inspired by True2Form: refining neural predictions with classical geometry</p>

    <div class="stats-grid">
        <div class="stat-card">
            <div class="stat-value">1</div>
            <div class="stat-label">Connectivity<br>Connect nearby endpoints</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">2</div>
            <div class="stat-label">Parallelism<br>Enforce parallel pairs</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">3</div>
            <div class="stat-label">Orthogonality<br>Enforce perpendicular strokes</div>
        </div>
    </div>

    <h3>Progressive Refinement</h3>
    <div class="viz-grid viz-grid-4">
        <div class="viz-item">
            <h4>2D Input</h4>
            <img src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/projection.png" alt="Input">
        </div>
        <div class="viz-item">
            <h4>Ground Truth</h4>
            <model-viewer
                src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/1_ground_truth_cad.glb"
                camera-controls camera-orbit="45deg 55deg auto" bounds="tight"
            ></model-viewer>
        </div>
        <div class="viz-item">
            <h4>Before Optimization</h4>
            <model-viewer
                src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/3_vggt_fitted_strokes.glb"
                camera-controls camera-orbit="45deg 55deg auto" bounds="tight"
            ></model-viewer>
        </div>
        <div class="viz-item">
            <h4>After Optimization</h4>
            <model-viewer
                src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/4_optimized_strokes.glb"
                camera-controls camera-orbit="45deg 55deg auto" bounds="tight"
            ></model-viewer>
        </div>
    </div>

    <h3 style="margin-top: 24px;">Constraint Stages</h3>
    <div class="stage-row">
        <div class="stage-item">
            <img src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/5_stage1_connectivity_constraints.png" alt="Connectivity">
            <div class="stage-label">Connectivity</div>
        </div>
        <div class="stage-item">
            <img src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/5_stage2_parallel_constraints.png" alt="Parallel">
            <div class="stage-label">Parallelism</div>
        </div>
        <div class="stage-item">
            <img src="https://yangxiaoxi65.github.io/VGGT_True2Form_Optimization/best_config_100scenes_20251020_145333/item_0/5_stage3_orthogonal_constraints.png" alt="Orthogonal">
            <div class="stage-label">Orthogonality</div>
        </div>
    </div>

    <div class="stats-grid" style="margin-top: 24px;">
        <div class="stat-card">
            <div class="stat-value green">+8.64%</div>
            <div class="stat-label">Mean Improvement</div>
        </div>
        <div class="stat-card">
            <div class="stat-value green">98/100</div>
            <div class="stat-label">Scenes Improved</div>
        </div>
        <div class="stat-card">
            <div class="stat-value green">+25.16%</div>
            <div class="stat-label">Best Case</div>
        </div>
    </div>

    <div class="card card-orange" style="margin-top: 20px;">
        <p style="margin-bottom:0;"><strong>Challenge:</strong> Tuning the optimizer was non-trivial. Early attempts produced coordinate explosions where strokes diverged to infinity. We systematically searched over constraint weights and connection thresholds to find a stable configuration.</p>
    </div>

    <div class="section-gap"></div>

    <!-- Stage 3: Fine-tuning -->
    <h2>Stage 3: Fine-tuning VGGT</h2>
    <p class="section-subtitle">Improving depth predictions with synthetic training data</p>

    <div class="card card-blue">
        <h3>Approach</h3>
        <p>VGGT was trained on photographs, not line drawings. We built a <strong>synthetic training data pipeline</strong>:</p>
        <ol style="margin-left: 20px; margin-top: 8px; font-size: 15px;">
            <li>Execute CAD construction sequences from Fusion 360 JSON to produce STEP files</li>
            <li>Extract precise wireframe edges from STEP geometry</li>
            <li>Render wireframe images with ground truth depth from multiple viewpoints</li>
        </ol>
        <p style="margin-top: 10px;">Generated <strong>100 scenes &times; 18 views = 1,800 training samples</strong>. Fine-tuned only the last conv layer (~37K of ~1B parameters).</p>
        <p style="margin-bottom: 0;"><strong>Exploration:</strong> We experimented with multiple training datasets (procedural shapes vs. Fusion 360 human designs) and fine-tuning strategies (which layers to freeze, learning rates). Freezing all but the final output layer gave the best balance between in-domain gains and generalization.</p>
    </div>

    <div class="stats-grid">
        <div class="stat-card">
            <div class="stat-value green">+9.5%</div>
            <div class="stat-label">Depth RMSE<br>(In-domain)</div>
        </div>
        <div class="stat-card">
            <div class="stat-value green">+26.8%</div>
            <div class="stat-label">Depth Accuracy<br>(Procrustes-aligned)</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">37K</div>
            <div class="stat-label">Parameters Trained<br>(of ~1B total)</div>
        </div>
    </div>

    <div class="section-gap"></div>

    <!-- Stage 4: Domain Gap -->
    <h2>Stage 4: The Domain Gap</h2>
    <p class="section-subtitle">The most interesting research finding</p>

    <div class="card card-red">
        <p>When evaluated on <strong>178 real hand-drawn sketches</strong> (OpenSketch dataset), the fine-tuned model performed <strong>worse</strong> than the original. Clean CAD wireframes differ significantly from messy human strokes.</p>
    </div>

    <div class="stats-grid" style="grid-template-columns: 1fr 1fr;">
        <div class="stat-card">
            <div class="stat-value green">Better</div>
            <div class="stat-label">Evaluated on Synthetic CAD<br><span class="badge badge-green">In-domain</span></div>
        </div>
        <div class="stat-card">
            <div class="stat-value red">-11.7%</div>
            <div class="stat-label">Evaluated on Real Sketches<br><span class="badge badge-red">Domain gap</span></div>
        </div>
    </div>

    <div class="card card-orange" style="margin-top: 20px;">
        <h3>Creative Solution: Flip the Approach</h3>
        <p>Instead of making our training data messier to match real sketches, we flipped the problem: use generative AI to make real sketches look like our clean training data at inference time.</p>
        <p style="margin-bottom:0;">We used <strong>Nano Banana Pro</strong> (Google Gemini 3 Pro Image) as a "de-stylization" pre-processor, feeding it a clean CAD wireframe as reference along with each hand-drawn sketch, and asking it to convert the sketch into clean CAD-like lines while preserving the original geometry. Applied to all 178 OpenSketch drawings, this yielded a <strong>+4.1% improvement</strong> over the original VGGT baseline.</p>
    </div>

    <div class="insight">
        <strong>Key Takeaway:</strong> The synthetic-to-real domain gap can be bridged from the inference side, not just the training side. Rather than collecting more diverse training data, pre-processing real inputs to match the training distribution is a practical and effective alternative.
    </div>

    <div class="section-gap"></div>

    <!-- Current & Future -->
    <h2>Current Work &amp; Future Directions</h2>
    <div class="direction-card">
        <div class="direction-icon">1</div>
        <div>
            <h3>Cross-dataset Generalization</h3>
            <p style="margin-bottom:0;">Train on one synthetic dataset (procedural), evaluate on another (Fusion 360 human designs) to verify improvements generalize.</p>
        </div>
    </div>
    <div class="direction-card">
        <div class="direction-icon">2</div>
        <div>
            <h3>Scaling Training Data</h3>
            <p style="margin-bottom:0;">Exploring larger datasets (CAD Recode, 1M models) and optimal fine-tuning strategies to find the best parameter-performance tradeoff.</p>
        </div>
    </div>
    <div class="direction-card">
        <div class="direction-icon">3</div>
        <div>
            <h3>Real Designer Drawings</h3>
            <p style="margin-bottom:0;">Collaborators are collecting hand-drawn sketches from industrial designers for full pipeline evaluation on realistic inputs.</p>
        </div>
    </div>

</div>
</div>

<!-- ==================== TAB 3: PROJECTS ==================== -->
<div class="tab-content" id="tab-projects">
<div class="container">

    <h2>Final Projects</h2>

    <!-- Fluid Simulation -->
    <div class="project-card">
        <div class="project-header">
            <h3>3D Interactive Fluid Simulation</h3>
            <div class="project-date">Apr - May 2025</div>
        </div>
        <div class="project-tech">
            <span class="tech-tag">C++</span>
            <span class="tech-tag">OpenGL</span>
            <span class="tech-tag">GLSL</span>
            <span class="tech-tag">CUDA</span>
        </div>
        <p style="font-size:14px; color:#888; margin-bottom:10px;">CSCI 2240 Advanced Computer Graphics Final Project</p>
        <ul>
            <li>Implemented grid-based Navier-Stokes fluid solver based on Jos Stam's Stable Fluids algorithm with mouse-based interactive fluid generation</li>
            <li>Developed ray marching volume renderer with 11 visual schemes using FBM noise; created fruit-shaped obstacle system with fluid-obstacle interaction</li>
            <li>Implemented vorticity confinement and shell rendering; optimized with CUDA GPU acceleration achieving up to <strong>6.7x speedup</strong></li>
        </ul>
        <video controls style="width:100%; border-radius:8px; margin-top:12px; aspect-ratio:16/9; background:#000;" preload="metadata">
            <source src="fluid_sim.mp4" type="video/mp4">
        </video>
    </div>

    <!-- Environmental Renderer -->
    <div class="project-card">
        <div class="project-header">
            <h3>Real-time Environmental Renderer</h3>
            <div class="project-date">Nov - Dec 2024</div>
        </div>
        <div class="project-tech">
            <span class="tech-tag">C++</span>
            <span class="tech-tag">OpenGL</span>
            <span class="tech-tag">GLSL</span>
        </div>
        <p style="font-size:14px; color:#888; margin-bottom:10px;">CSCI 1230 Computer Graphics Final Project</p>
        <ul>
            <li>Developed real-time renderer with infinite procedural terrain generation, day/night cycle, and 3 distinct terrain biomes</li>
            <li>Implemented Preetham Sky Model for atmospheric scattering; designed water system with 3-layer displacement mapping and dynamic UV animation</li>
            <li>Created GPU-based particle system for rain/snow effects; used Perlin noise terrain with height-based texture blending and multi-threaded chunk loading</li>
        </ul>
        <video controls style="width:100%; border-radius:8px; margin-top:12px; aspect-ratio:16/9; background:#000;" preload="metadata">
            <source src="naturewerks.mp4" type="video/mp4">
        </video>
    </div>

    <!-- Animatable Avatar -->
    <div class="project-card">
        <div class="project-header">
            <h3>Animatable Avatar from Monocular Video</h3>
            <div class="project-date">Apr - May 2025</div>
        </div>
        <div class="project-tech">
            <span class="tech-tag">Python</span>
            <span class="tech-tag">PyTorch</span>
            <span class="tech-tag">SAM2</span>
            <span class="tech-tag">SMPL-X</span>
            <span class="tech-tag">3DGS</span>
        </div>
        <p style="font-size:14px; color:#888; margin-bottom:10px;">CSCI 1430 Computer Vision Final Project</p>
        <ul>
            <li>Built 4-stage motion transfer pipeline: OpenPose 2D pose estimation, SAM2 segmentation, SMPL-X 3D mesh reconstruction, and 3D Gaussian Splatting rendering</li>
            <li>Successfully transferred dance movements between actors from monocular video input</li>
            <li>Resolved VPoser-to-SMPL-X parameter compatibility issues; developed data conversion utilities and visualization tools</li>
        </ul>
        <img src="poster_avatar.png" alt="Animation Avatars Poster" style="width:100%; border-radius:8px; margin-top:12px; border:1px solid #d0dcde;">
    </div>

    <div class="section-gap"></div>

    <h2>Course Projects</h2>

    <div>
        <div class="assignment-card">
            <div class="img-wrapper"><img src="path_refraction.png" alt="Path Tracing" class="assignment-img"></div>
            <div>
                <div class="school">Path Tracing</div>
                <div class="degree">Diffuse, glossy, mirror, and refraction BRDFs with direct lighting importance sampling, Russian roulette, and Extended Reinhard tone mapping</div>
            </div>
        </div>
        <div class="assignment-card">
            <div class="img-wrapper"><img src="mesh_simplify.png" alt="Geometry Processing" class="assignment-img"></div>
            <div>
                <div class="school">Geometry Processing</div>
                <div class="degree">Half-edge mesh data structure, Loop subdivision, Quadric Error simplification, isotropic remeshing, and bilateral mesh denoising</div>
            </div>
        </div>
        <div class="assignment-card">
            <div class="img-wrapper"><img src="arap_armadillo.gif" alt="ARAP Deformation" class="assignment-img"></div>
            <div>
                <div class="school">ARAP Deformation</div>
                <div class="degree">Cotangent-weight Laplacian, SVD-based rotation fitting, sparse linear system solving, with rotation propagation and parallelized optimization</div>
            </div>
        </div>
        <div class="assignment-card">
            <div class="img-wrapper"><video src="fem_sim.mov" autoplay loop muted playsinline class="assignment-img"></video></div>
            <div>
                <div class="school">FEM Simulation</div>
                <div class="degree">Finite element elastic simulation with Green's strain, collision resolution, RK4 and Velocity Verlet integration, adaptive timestepping</div>
            </div>
        </div>
    </div>

</div>
</div>

<!-- ==================== TAB 4: PUBLICATIONS ==================== -->
<div class="tab-content" id="tab-publications">
<div class="container">

    <h2>Publications</h2>

    <div class="pub-card">
        <div class="pub-title">Bimodal Affective Computing Interfaces for Emerging Artificial Intelligence Paradigms</div>
        <div class="pub-authors">I. Gonsher, J. Phelps, S. Yan, <strong>X. Yang</strong></div>
        <div class="pub-venue">Proc. Int. Conf. Applied Human Factors and Ergonomics (AHFE), 2024 &middot; <a href="https://doi.org/10.54941/ahfe1005363" target="_blank" style="color:#2a6269;">Paper</a></div>
        <p style="margin-top: 10px; font-size: 15px;">Proposed the Bimodal Affective Computing (BAC) paradigm, where both humans and machines can recognize and respond to each other's affective states. Built a mouse prototype with Inflatable Responsive Interfaces (IRI) that expand and contract based on CPU usage, using Arduino-controlled air pressure sensors and peristaltic pumps to create a tangible sense of the computer "breathing."</p>
    </div>

    <div class="pub-card">
        <div class="pub-title">Enhancing Few-Shot 3D Point Cloud Semantic Segmentation through Bidirectional Prototype Learning</div>
        <div class="pub-authors">X. Guo, H. Hu, <strong>X. Yang</strong>, Y. Deng</div>
        <div class="pub-venue">Proc. 9th Int. Conf. Robotics and Artificial Intelligence (ICRAI), ACM, 2023 &middot; <a href="https://doi.org/10.1145/3637843.3637848" target="_blank" style="color:#2a6269;">Paper</a></div>
        <p style="margin-top: 10px; font-size: 15px;">Introduced bidirectional prototype learning for few-shot 3D point cloud segmentation. Unlike prior methods that only generate prototypes from the support set, our method learns prototypes in both forward (support-to-query) and backward (query-to-support) directions using a DGCNN backbone with edge convolution and self-attention. Evaluated on S3DIS under 2-way 1-shot and 2-way 5-shot settings.</p>
    </div>

    <div class="pub-card">
        <div class="pub-title">Deep Neural Networks for Chinese Traditional Landscape Painting Creation</div>
        <div class="pub-authors"><strong>X. Yang</strong>, J. Hu</div>
        <div class="pub-venue">Proc. SPIE, 2nd Int. Conf. Artificial Intelligence, Automation and High-Performance Computing, 2022 &middot; <a href="https://doi.org/10.1117/12.2641585" target="_blank" style="color:#2a6269;">Paper</a></div>
        <p style="margin-top: 10px; font-size: 15px;">Applied two deep learning approaches to generate traditional Chinese landscape paintings: VGG19-based neural style transfer to render photos in Chinese painting style, and DCGAN to synthesize new paintings from scratch. Trained on 1,301 paintings from the Smithsonian's Freer Gallery of Art.</p>
    </div>

</div>
</div>


<!-- Lightbox overlay -->
<div class="lightbox-overlay" id="lightbox" onclick="closeLightbox()">
</div>

<!-- Tab switching script -->
<script>
function switchTab(tabName) {
    // Hide all tabs
    document.querySelectorAll('.tab-content').forEach(el => el.classList.remove('active'));
    document.querySelectorAll('.tab-btn').forEach(el => el.classList.remove('active'));

    // Show selected tab
    document.getElementById('tab-' + tabName).classList.add('active');

    // Activate button
    const buttons = document.querySelectorAll('.tab-btn');
    const tabNames = ['about', 'research', 'projects', 'publications'];
    const idx = tabNames.indexOf(tabName);
    if (idx >= 0) buttons[idx].classList.add('active');

    // Scroll to top of content
    window.scrollTo(0, 0);
}

// Lightbox
function openLightbox(el) {
    const overlay = document.getElementById('lightbox');
    overlay.innerHTML = '';
    if (el.tagName === 'VIDEO') {
        const v = document.createElement('video');
        v.src = el.src || el.querySelector('source')?.src;
        v.controls = true;
        v.autoplay = true;
        v.style.maxWidth = '90vw';
        v.style.maxHeight = '90vh';
        v.onclick = function(e) { e.stopPropagation(); };
        overlay.appendChild(v);
    } else {
        const img = document.createElement('img');
        img.src = el.src;
        overlay.appendChild(img);
    }
    overlay.classList.add('active');
}
function closeLightbox() {
    const overlay = document.getElementById('lightbox');
    overlay.classList.remove('active');
    const v = overlay.querySelector('video');
    if (v) v.pause();
    overlay.innerHTML = '';
}
document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape') closeLightbox();
});
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.assignment-img, .project-card img').forEach(function(el) {
        el.addEventListener('click', function(e) {
            e.preventDefault();
            openLightbox(el);
        });
    });
});
</script>

<footer style="text-align:center; font-size:12px; color:#aaa; padding:24px 0 16px;">Last updated: February 2026</footer>

</body>
</html>
